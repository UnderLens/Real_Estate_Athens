{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bff0f8d9",
   "metadata": {},
   "source": [
    "# Real Estate Price Prediction with Deep Neural Network\n",
    "This notebook demonstrates the process of training a Deep Neural Network (DNN) to predict real estate prices using a dataset of listings in Greece. The dataset is preprocessed, a model is trained, and then predictions are made on new data.\n",
    "In this updated version, we include hyperparameter tuning and cross-validation to improve the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3487353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the cleaned dataset\n",
    "data_path = r\"C:\\cleaned_greece_listings.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc9ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "features = [\n",
    "    'location_name', 'location_region', 'res_type', 'res_sqr', \n",
    "    'construction_year', 'bedrooms', 'bathrooms', 'auto_heating', \n",
    "    'solar', 'cooling', 'safe_door', 'gas', 'fireplace', 'furniture', 'student'\n",
    "]\n",
    "X = data[features]\n",
    "y = data['res_price']\n",
    "\n",
    "# Define numeric and categorical features\n",
    "numeric_features = ['res_sqr', 'construction_year', 'bedrooms', 'bathrooms']\n",
    "categorical_features = ['location_name', 'location_region', 'res_type']\n",
    "\n",
    "# Create pipelines for preprocessing\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Combine the pipelines into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Fit the preprocessor on the training data\n",
    "preprocessor.fit(X)\n",
    "\n",
    "# Save the preprocessor\n",
    "joblib.dump(preprocessor, 'preprocessor.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05076aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training data\n",
    "X_train_preprocessed = preprocessor.transform(X)\n",
    "\n",
    "# Convert the transformed data to a dense array if necessary\n",
    "X_train_preprocessed = X_train_preprocessed.toarray()\n",
    "\n",
    "# Define the model\n",
    "def build_model(learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=[X_train_preprocessed.shape[1]]),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Implement early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with different batch sizes and learning rates\n",
    "batch_sizes = [16, 32, 64]\n",
    "learning_rates = [0.001, 0.0005, 0.0001]\n",
    "best_model = None\n",
    "best_mae = np.inf\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        print(f\"Training with batch size {batch_size} and learning rate {lr}...\")\n",
    "        model = build_model(learning_rate=lr)\n",
    "        history = model.fit(X_train_preprocessed, y, validation_split=0.2, epochs=100, batch_size=batch_size, callbacks=[early_stopping], verbose=0)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        val_mae = min(history.history['val_mae'])\n",
    "        if val_mae < best_mae:\n",
    "            best_mae = val_mae\n",
    "            best_model = model\n",
    "\n",
    "print(f\"Best model has MAE: {best_mae}\")\n",
    "\n",
    "# Define K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cross_val_mae = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_preprocessed):\n",
    "    X_train_fold, X_val_fold = X_train_preprocessed[train_index], X_train_preprocessed[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "    model = build_model(learning_rate=0.001)\n",
    "    history = model.fit(X_train_fold, y_train_fold, validation_data=(X_val_fold, y_val_fold), epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0)\n",
    "    \n",
    "    # Evaluate on the validation fold\n",
    "    val_mae = min(history.history['val_mae'])\n",
    "    cross_val_mae.append(val_mae)\n",
    "\n",
    "print(f\"Cross-Validation MAE: {np.mean(cross_val_mae)}\")\n",
    "\n",
    "# Save the best model\n",
    "best_model.save('real_estate_price_predictor_best.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152b158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved preprocessor\n",
    "preprocessor = joblib.load('preprocessor.pkl')\n",
    "\n",
    "# Load the best model from hyperparameter tuning and cross-validation\n",
    "model = load_model('real_estate_price_predictor_best.h5')\n",
    "\n",
    "# Load new data for prediction (for demonstration, using the same training data)\n",
    "# In practice, this should be new, unseen data\n",
    "X_new = X\n",
    "\n",
    "# Preprocess the new data\n",
    "X_new_preprocessed = preprocessor.transform(X_new)\n",
    "X_new_preprocessed = X_new_preprocessed.toarray()\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_new_preprocessed)\n",
    "\n",
    "# Compare predictions to actual prices\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(predictions, label='Predicted Prices')\n",
    "plt.plot(y.values, label='Actual Prices')\n",
    "plt.title('Real Estate Price Predictions')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
